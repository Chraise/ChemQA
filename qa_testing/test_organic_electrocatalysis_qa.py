#!/usr/bin/env python3
"""
æœ‰æœºç”µå‚¬åŒ–é—®ç­”æµ‹è¯•è„šæœ¬

ç”Ÿæˆ10ä¸ªè‹±æ–‡çš„æœ‰æœºç”µå‚¬åŒ–é¢†åŸŸé—®é¢˜ï¼Œè·å–å›ç­”å¹¶è¿›è¡Œå¼•ç”¨åˆ†æã€‚
"""

import sys
import os
import json
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.qa_system.expert_system import ChemicalQAExpert
from src.qa_system.citation_analyzer import analyze_answer_citations


class OrganicElectrocatalysisQATester:
    """æœ‰æœºç”µå‚¬åŒ–é—®ç­”æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.expert = ChemicalQAExpert()
        self.results = []
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("test_output")
        self.output_dir.mkdir(exist_ok=True)
        
        # å®šä¹‰10ä¸ªæœ‰æœºç”µå‚¬åŒ–é¢†åŸŸçš„é—®é¢˜
        self.questions = [
            "What are the key mechanisms of CO2 electroreduction on copper-based catalysts?",
            "How does the coordination environment affect the performance of nickel-based electrocatalysts for alcohol oxidation?",
            "What are the recent advances in electrochemical C-H functionalization of aromatic compounds?",
            "How can we improve the selectivity of electrochemical reduction of carbonyl compounds?",
            "What role do electrolyte additives play in organic electrocatalysis?",
            "How does the surface structure of platinum catalysts influence organic molecule oxidation?",
            "What are the challenges and solutions for electrochemical synthesis of heterocyclic compounds?",
            "How can we achieve high current density in organic electrosynthesis?",
            "What are the mechanistic insights into electrochemical oxidation of biomass-derived compounds?",
            "How does the pH of electrolyte affect the reaction pathways in organic electrocatalysis?"
        ]
    
    def run_single_test(self, question: str, question_id: int) -> dict:
        """è¿è¡Œå•ä¸ªé—®ç­”æµ‹è¯•"""
        print(f"\n{'='*80}")
        print(f"æµ‹è¯• {question_id}/10: {question}")
        print(f"{'='*80}")
        
        try:
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            # è·å–ç­”æ¡ˆï¼ˆä¸è¿›è¡Œå¼•ç”¨åˆ†æï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åŸå§‹ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡ï¼‰
            print("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
            
            # æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
            context = self.expert.retriever.retrieve_relevant_context(question, return_dict_list=True)
            
            # ç”ŸæˆåŸå§‹ç­”æ¡ˆ
            raw_answer = self.expert.generator.generate_answer(question, context)
            
            # åˆ†æå¼•ç”¨æƒ…å†µ
            print("æ­£åœ¨åˆ†æå¼•ç”¨æƒ…å†µ...")
            analysis_report = analyze_answer_citations(
                answer=raw_answer,
                context=context,
                print_report=False,  # ä¸æ‰“å°è¯¦ç»†æŠ¥å‘Šï¼Œåªè¿”å›ç»“æœ
                save_to_file=False
            )
            
            # æ ¼å¼åŒ–ç­”æ¡ˆ
            formatted_response = self.expert.formatter.format(question, raw_answer, context)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = time.time() - start_time
            
            # ä¿å­˜ç»“æœ
            result = {
                "question_id": question_id,
                "question": question,
                "processing_time": round(processing_time, 2),
                "timestamp": datetime.now().isoformat(),
                "citation_analysis": analysis_report,
                "context_count": len(context),
                "answer_length": len(raw_answer),
                "formatted_answer": formatted_response
            }
            
            # ä¿å­˜å•ä¸ªæµ‹è¯•çš„ç­”æ¡ˆæ–‡ä»¶
            answer_file = self.output_dir / f"answer_{question_id:02d}.md"
            with open(answer_file, 'w', encoding='utf-8') as f:
                f.write(formatted_response)
            
            # ä¿å­˜å•ä¸ªæµ‹è¯•çš„å¼•ç”¨åˆ†æ
            analysis_file = self.output_dir / f"citation_analysis_{question_id:02d}.txt"
            with open(analysis_file, 'w', encoding='utf-8') as f:
                f.write(f"Question {question_id}: {question}\n")
                f.write("="*60 + "\n\n")
                f.write(f"Processing Time: {processing_time:.2f} seconds\n")
                f.write(f"Context Chunks: {len(context)}\n")
                f.write(f"Answer Length: {len(raw_answer)} characters\n\n")
                f.write("Citation Analysis Summary:\n")
                f.write(f"- Total Context Chunks: {analysis_report['total_context_chunks']}\n")
                f.write(f"- Cited Chunks: {analysis_report['citation_coverage']['cited_count']}\n")
                f.write(f"- Citation Coverage: {analysis_report['citation_coverage']['coverage_percentage']}%\n")
                f.write(f"- Unused Chunks: {analysis_report['citation_coverage']['unused_count']}\n\n")
                
                if analysis_report['cited_chunks_analysis']['cited_chunks']:
                    f.write("Cited Chunks Details:\n")
                    for i, chunk in enumerate(analysis_report['cited_chunks_analysis']['cited_chunks'], 1):
                        f.write(f"{i}. Source: {chunk['source']}\n")
                        f.write(f"   Chunk Index: {chunk['chunk_index']}\n")
                        f.write(f"   Citation Count: {chunk['citation_count']}\n")
                        f.write(f"   Content Preview: {chunk['text_preview']}\n\n")
            
            print(f"âœ… æµ‹è¯• {question_id} å®Œæˆ")
            print(f"   å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            print(f"   ä¸Šä¸‹æ–‡ç‰‡æ®µ: {len(context)} ä¸ª")
            print(f"   å¼•ç”¨è¦†ç›–ç‡: {analysis_report['citation_coverage']['coverage_percentage']}%")
            
            return result
            
        except Exception as e:
            print(f"âŒ æµ‹è¯• {question_id} å¤±è´¥: {str(e)}")
            return {
                "question_id": question_id,
                "question": question,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æœ‰æœºç”µå‚¬åŒ–é—®ç­”æµ‹è¯•")
        print(f"ğŸ“ æ€»å…± {len(self.questions)} ä¸ªé—®é¢˜")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        
        start_time = time.time()
        
        for i, question in enumerate(self.questions, 1):
            result = self.run_single_test(question, i)
            self.results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
            if i < len(self.questions):
                print("â³ ç­‰å¾…3ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")
                time.sleep(3)
        
        total_time = time.time() - start_time
        
        # ä¿å­˜æ±‡æ€»ç»“æœ
        self.save_summary_results(total_time)
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"ğŸ“Š ç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
    
    def save_summary_results(self, total_time: float):
        """ä¿å­˜æ±‡æ€»ç»“æœ"""
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_tests = [r for r in self.results if 'error' not in r]
        failed_tests = [r for r in self.results if 'error' in r]
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_processing_time = sum(r['processing_time'] for r in successful_tests) / len(successful_tests) if successful_tests else 0
        avg_context_count = sum(r['context_count'] for r in successful_tests) / len(successful_tests) if successful_tests else 0
        avg_citation_coverage = sum(r['citation_analysis']['citation_coverage']['coverage_percentage'] for r in successful_tests) / len(successful_tests) if successful_tests else 0
        
        # ä¿å­˜JSONæ ¼å¼çš„æ±‡æ€»ç»“æœ
        summary_data = {
            "test_info": {
                "total_questions": len(self.questions),
                "successful_tests": len(successful_tests),
                "failed_tests": len(failed_tests),
                "total_time": round(total_time, 2),
                "timestamp": datetime.now().isoformat()
            },
            "statistics": {
                "avg_processing_time": round(avg_processing_time, 2),
                "avg_context_count": round(avg_context_count, 1),
                "avg_citation_coverage": round(avg_citation_coverage, 2)
            },
            "detailed_results": self.results
        }
        
        # ä¿å­˜JSONæ–‡ä»¶
        json_file = self.output_dir / "test_summary.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ–‡æœ¬æ ¼å¼çš„æ±‡æ€»æŠ¥å‘Š
        report_file = self.output_dir / "test_summary_report.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("æœ‰æœºç”µå‚¬åŒ–é—®ç­”æµ‹è¯•æ±‡æ€»æŠ¥å‘Š\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»é—®é¢˜æ•°: {len(self.questions)}\n")
            f.write(f"æˆåŠŸæµ‹è¯•: {len(successful_tests)}\n")
            f.write(f"å¤±è´¥æµ‹è¯•: {len(failed_tests)}\n")
            f.write(f"æ€»è€—æ—¶: {total_time:.2f} ç§’\n\n")
            
            f.write("ç»Ÿè®¡ä¿¡æ¯:\n")
            f.write(f"- å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f} ç§’\n")
            f.write(f"- å¹³å‡ä¸Šä¸‹æ–‡ç‰‡æ®µæ•°: {avg_context_count:.1f}\n")
            f.write(f"- å¹³å‡å¼•ç”¨è¦†ç›–ç‡: {avg_citation_coverage:.2f}%\n\n")
            
            f.write("è¯¦ç»†ç»“æœ:\n")
            for result in self.results:
                f.write(f"\né—®é¢˜ {result['question_id']}: {result['question']}\n")
                if 'error' in result:
                    f.write(f"çŠ¶æ€: å¤±è´¥ - {result['error']}\n")
                else:
                    f.write(f"çŠ¶æ€: æˆåŠŸ\n")
                    f.write(f"å¤„ç†æ—¶é—´: {result['processing_time']} ç§’\n")
                    f.write(f"ä¸Šä¸‹æ–‡ç‰‡æ®µ: {result['context_count']} ä¸ª\n")
                    f.write(f"å¼•ç”¨è¦†ç›–ç‡: {result['citation_analysis']['citation_coverage']['coverage_percentage']}%\n")
                    f.write(f"ç­”æ¡ˆé•¿åº¦: {result['answer_length']} å­—ç¬¦\n")
        
        # æ‰“å°æ±‡æ€»ä¿¡æ¯
        print(f"\nğŸ“ˆ æµ‹è¯•æ±‡æ€»:")
        print(f"   æˆåŠŸ: {len(successful_tests)}/{len(self.questions)}")
        print(f"   å¤±è´¥: {len(failed_tests)}/{len(self.questions)}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f} ç§’")
        print(f"   å¹³å‡å¼•ç”¨è¦†ç›–ç‡: {avg_citation_coverage:.2f}%")
        
        if failed_tests:
            print(f"\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for test in failed_tests:
                print(f"   é—®é¢˜ {test['question_id']}: {test['error']}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª æœ‰æœºç”µå‚¬åŒ–é—®ç­”æµ‹è¯•è„šæœ¬")
    print("="*60)
    
    # åˆ›å»ºæµ‹è¯•å™¨å¹¶è¿è¡Œæµ‹è¯•
    tester = OrganicElectrocatalysisQATester()
    tester.run_all_tests()


if __name__ == "__main__":
    main() 